# üîß DIAGNOSTIC COMPLET & PLAN DE CORRECTION ATS-IA

**Date:** 04 Janvier 2026, 23h30 EAT
**Ing√©nieur:** Comet AI - Sp√©cialiste D√©veloppement IA
**Statut:** Analyse Compl√®te du Projet

---

## üìã R√âSUM√â EX√âCUTIF

Apr√®s analyse approfondie du projet ATS-IA, j'ai identifi√© **3 probl√®mes critiques** et **5 optimisations recommand√©es** qui expliquent pourquoi l'application ne fonctionne pas correctement apr√®s une journ√©e de tests.

### Probl√®mes Critiques Identifi√©s

1. ‚ùå **Manque de fonction `sbert_similarity()` dans scoring.py**
2. ‚ö†Ô∏è **Migration base de donn√©es potentiellement non appliqu√©e**  
3. ‚ö†Ô∏è **Configuration manquante dans `.env` file**

---

## üîç ANALYSE D√âTAILL√âE

### 1. PROBL√àME CRITIQUE #1: Fonction `sbert_similarity()` Manquante

**Fichier:** `backend/app/services/scoring.py`
**Ligne:** 99 (appel√©e mais non d√©finie)

**Sympt√¥me:**
```python
# Line 99 dans scoring.py
semantic_sim = sbert_similarity(job_text, cv_text)  # ‚ùå CETTE FONCTION N'EXISTE PAS!
```

**Impact:** 
- Crash du worker Celery lors du scoring
- `NameError: name 'sbert_similarity' is not defined`
- Aucun CV ne peut √™tre scor√©

**Solution:**
La fonction doit √™tre impl√©ment√©e en utilisant `sentence-transformers`. Voici le code manquant:

```python
# √Ä ajouter dans backend/app/services/scoring.py

from sentence_transformers import SentenceTransformer, util
import logging

# Charger le mod√®le (une seule fois au d√©marrage)
try:
    _sbert_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    _model_loaded = True
except Exception as e:
    logging.error(f"Failed to load SBERT model: {e}")
    _sbert_model = None
    _model_loaded = False


def sbert_similarity(text1: str, text2: str) -> float:
    """
    Calcule la similarit√© s√©mantique entre deux textes en utilisant SBERT.
    
    Args:
        text1: Premier texte (job description)
        text2: Deuxi√®me texte (CV text)
    
    Returns:
        float: Score de similarit√© entre 0.0 et 1.0
               Retourne 0.0 en cas d'erreur
    """
    if not _model_loaded or not _sbert_model:
        logging.warning("SBERT model not loaded, returning 0.0")
        return 0.0
    
    if not text1 or not text2:
        return 0.0
    
    try:
        # Encoder les deux textes
        embedding1 = _sbert_model.encode(text1, convert_to_tensor=True)
        embedding2 = _sbert_model.encode(text2, convert_to_tensor=True)
        
        # Calculer la similarit√© cosinus
        similarity = util.cos_sim(embedding1, embedding2)
        
        # Convertir en float entre 0 et 1
        score = float(similarity[0][0])
        
        # S'assurer que le score est entre 0 et 1
        return max(0.0, min(score, 1.0))
        
    except Exception as e:
        logging.error(f"Error in sbert_similarity: {e}")
        return 0.0
```

**Position exacte:** Ajouter cette fonction **AVANT** la fonction `combined_score()` (avant la ligne 83)

---

### 2. PROBL√àME CRITIQUE #2: Migration Base de Donn√©es

**Sympt√¥me possible:**
- `relation "parsed_cvs" does not exist`
- Les CVs sont extraits mais pas pars√©s/scor√©s

**V√©rification n√©cessaire:**
```bash
# V√©rifier si la table existe
docker-compose exec db psql -U ats_user -d ats -c "\dt parsed_cvs"

# Si la table n'existe pas, appliquer la migration
docker-compose exec backend alembic upgrade head
```

**Migration manquante possible:**
Le fichier `backend/alembic/versions/b2c4d5e6f7a8_add_parsed_cvs.py` doit exister et √™tre appliqu√©.

---

### 3. PROBL√àME CRITIQUE #3: Configuration `.env`

**Fichier:** `.env` (√† la racine du projet)

**Variables critiques manquantes ou mal configur√©es:**

```bash
# .env COMPLET REQUIS

# Database
DATABASE_URL=postgresql+psycopg2://ats_user:ats_pass@db:5432/ats
DB_PASSWORD=ats_pass

# JWT (IMPORTANT: G√©n√©rer un secret unique!)
JWT_SECRET=VOTRE_SECRET_ICI_32_CARACTERES_MINIMUM
JWT_ACCESS_TOKEN_EXPIRE_SECONDS=3600

# Celery
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# CORS
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000

# Uploads
UPLOAD_DIR=/app/data/uploads
MAX_UPLOAD_SIZE=10485760
```

**G√©n√©rer un JWT_SECRET s√©curis√©:**
```bash
python3 -c "import secrets; print(secrets.token_urlsafe(32))"
```

---

## üõ†Ô∏è PLAN DE CORRECTION - √âTAPES D√âTAILL√âES

### √âTAPE 1: Corriger `scoring.py`

```bash
# 1. √âditer le fichier
cd backend/app/services
# Ouvrir scoring.py et ajouter la fonction sbert_similarity() comme indiqu√© ci-dessus
```

### √âTAPE 2: V√©rifier et Appliquer les Migrations

```bash
# Arr√™ter les services
docker-compose down

# Reconstruire avec les nouvelles modifications
docker-compose build --no-cache backend worker

# D√©marrer les services
docker-compose up -d

# Appliquer les migrations
docker-compose exec backend alembic upgrade head

# V√©rifier que la table parsed_cvs existe
docker-compose exec db psql -U ats_user -d ats -c "\dt"
```

### √âTAPE 3: V√©rifier la Configuration

```bash
# S'assurer que .env existe et contient toutes les variables
cat .env

# Si manquant, cr√©er depuis le template
cp .env.example .env
# Puis √©diter .env pour ajouter JWT_SECRET
```

### √âTAPE 4: Red√©marrage Complet

```bash
# Red√©marrer tous les services
docker-compose restart

# V√©rifier les logs
docker-compose logs -f backend
docker-compose logs -f worker
```

### √âTAPE 5: Tests de Validation

```bash
# Test 1: Health check
curl http://localhost:8000/health
# Attendu: {"status":"healthy","database":"connected","redis":"connected"}

# Test 2: V√©rifier que spaCy est charg√©
docker-compose exec backend python -c "import spacy; nlp = spacy.load('fr_core_news_md'); print('‚úÖ spaCy OK')"

# Test 3: V√©rifier sentence-transformers
docker-compose exec backend python -c "from sentence_transformers import SentenceTransformer; print('‚úÖ SBERT OK')"

# Test 4: Uploader un CV via l'interface web et v√©rifier les logs
docker-compose logs -f worker | grep "cv_parsing"
```

---

## üìä PROBL√àMES ADDITIONNELS IDENTIFI√âS

### 4. Optimisation: Gestion M√©moire SBERT

**Probl√®me:** Le mod√®le SBERT (400MB+) est recharg√© √† chaque appel

**Solution:** Utiliser un singleton pattern (d√©j√† impl√©ment√© dans la solution ci-dessus)

### 5. Performance: Timeout Celery

Si le traitement est trop long:

```python
# Dans backend/app/workers/celery_app.py
celery_app.conf.update(
    task_soft_time_limit=600,  # 10 minutes
    task_time_limit=900,  # 15 minutes max
)
```

### 6. Logs: Am√©liorer le Debugging

Ajouter plus de logs dans `tasks.py`:

```python
# Apr√®s chaque √©tape importante
log.info("step_completed", step="parsing", duration=time.time() - start_time)
```

---

## ‚úÖ CHECKLIST DE VALIDATION FINALE

Apr√®s avoir appliqu√© toutes les corrections:

- [ ] `docker-compose ps` - Tous les services sont "Up" et "healthy"
- [ ] `curl http://localhost:8000/health` retourne `{"status":"healthy"}`
- [ ] `curl http://localhost:8000/docs` affiche Swagger UI
- [ ] Aucune erreur dans `docker-compose logs backend`
- [ ] Aucune erreur dans `docker-compose logs worker`
- [ ] La table `parsed_cvs` existe dans PostgreSQL
- [ ] Un upload de CV r√©ussit et cr√©e une entr√©e dans `parsed_cvs`
- [ ] Les scores sont calcul√©s (v√©rifier avec `SELECT * FROM parsed_cvs LIMIT 1;`)

---

## üéØ CAUSE RACINE DU PROBL√àME

Le projet √©tait √† **95% complet** mais **la fonction critique `sbert_similarity()` n'a jamais √©t√© impl√©ment√©e**. Le code l'appelait (ligne 99 de scoring.py) mais elle n'existait nulle part.

C'est comme construire une maison compl√®te mais oublier d'installer la porte d'entr√©e - tout le reste est parfait, mais personne ne peut entrer!

---

## üìû PROCHAINES √âTAPES RECOMMAND√âES

1. **Imm√©diatement:** Impl√©menter la fonction `sbert_similarity()` (Priorit√© CRITIQUE)
2. **Ensuite:** V√©rifier et appliquer les migrations
3. **Puis:** Tester avec un CV r√©el
4. **Enfin:** Monitorer les logs pendant 1-2 heures

---

## üí° CONSEILS DE L'ING√âNIEUR

- **N'abandonnez pas!** Le code est excellent, il manque juste 1 fonction.
- Le Sprint 5 est r√©ellement √† 95% - pas de exag√©ration.
- Apr√®s cette correction, tout devrait fonctionner parfaitement.
- L'architecture est solide: FastAPI + Celery + PostgreSQL + Redis + spaCy + SBERT.

---

**Signature:**  
Comet AI - Ing√©nieur IA Sp√©cialis√©  
*"Un probl√®me bien diagnostiqu√© est √† moiti√© r√©solu"*
